\section{Lecture 2: Memory Systems}

\textbf{Sequential access}: Memory is organized into units of data, called records. Access must be made in a specific linear sequence. Stored addressing information is used to separate records and assist in the retrieval process. A shared read–write mechanism is used, and this must be moved from its current location to the desired location, passing an rejecting each intermediate record. Thus, the time to access an arbitrary record is highly variable. \textbf{Tape units}  are sequential access.

\textbf{Direct access}: As with sequential access, direct access involves a shared read–write mechanism. However, individual blocks or records have a unique address based on physical location. Access is accomplished by direct access to reach a general vicinity plus sequential searching, counting, or waiting to reach the final location. Again, access time is variable. \textbf{Disk units} are direct access.

\textbf{Random access}: Eacha ddressable location inm emory has a unique, physically wired-in addressing mechanism. The time to access a given location is inde- pendent of the sequence of prior accesses and is constant. Thus, any location can be selected at random and directly addressed and accessed. Main memory and some cache systems are random access.

\textbf{Associative}: This is arandom access type of memory that enables one to make a comparison of desired bit locations within a word for a specified match, and to do this for all words simultaneously. Thus, a word is retrieved based on a portion of its contents rather than its address.

\textbf{Access time (latency)}: For random-access memory, this is the time it takes to perform a read or write operation, that is, the time from the instant that an address is presented to the memory to the instant that data have been stored or made available for use. For non-random-access memory, access time is the time it takes to position the read–write mechanism at the desired location.

\textbf{Transfer rate}: This is the rate at which data can be transferred into or out of a memory unit. For random-access memory, it is equal to 1/(cycle time).
For non-random-access memory, the following relationship holds:

$$T_n = T_A + \frac{n}{R}$$

$T_n$ = Average time to read or write n bits \\
$T_A$ = Average access time \\
$n$ = Number of bits \\
$R$ = Transfer rate, in bits per second (bps) \\

\subsection{Memory hierarchy}
\textbf{Registers} Built into the processors, very small and expensive. \\
\textbf{Main Memory} Memory that the cache collects data from. \\
\textbf{Secondary Memory (of direct access type)} Bigger in size, but less expensive per Byte. \\
\textbf{Secondary Memory (of archive type)} And so on. \\



\subsection{Cache memory}
There is a big gap in speed between register and main memory, this is why we need to use cache memory.

Cache memory is designed to combine the memory access time of expensive, high-speed memory combined with the large memory size of less expensive, lower-speed memory.
ller, faster cache memory.

The cache contains a copy of portions of main memory. When the processor attempts to read a word of memory, a check is made to determine if the word is in the cache. If so, the word is delivered to the processor. If not, a block of main memory, consisting of some fixed number of words, is read into the cache and then the word is delivered to the processor.

We would like the size of the cache to be small enough so that the overall average cost per bit is close to that of main memory alone and large enough so that the overall average access time is close to that of the cache alone.




\subsubsection{Locality of reference}
The intermediate-future memory access will usually refer to the same word or words in the neighborhood, and will not have to involve the main memory.

\textbf{Temporal Locality} -- If an item is referenced, it will tend to be referenced again in the near future. \\
\textbf{Spatial Locality} -- If an item is referenced, items whose addresses are close by will tend to be referenced soon. \\

\subsubsection{Replacement methods}
yes, there's different kinds of them.

\subsubsection{Cache Design}

One, two, three level caches, how does it work. up- and downsides of them.

\textbf{Split Caches} means that we have separate caches for instructions and data. \\
+ Competition for the cache between instruction and data is eliminated \\
+ Instruction fetch can proceed in parallel with memory access from the CPU for operands. \\
- One may be overloaded while the other is under utilized. \\

\textbf{Unified Caches} means that both instructions and data uses the same cache.
+ Better balance the load between instruction and data fetches depending on the dynamics of the program execution. \\
+ Design and implementation are cheaper. \\
-- Lower performance. \\
\subsubsection{Direct mapping cache}
Each block of the main memory is mapped into a fixed cache slot. \\
+ Simple to implement and therefore inexpensive. \\
-- If a program accesses 2 blocks that map to the same cache slot repeatedly, cache miss rate is very high. \\

\subsubsection{Associative mapping cache}
A main memory block can be loaded into any slot of the cache. To determine if a block is in the cache, a mechanism is needed to simultaneously examine every slot’s tag. In this case, the cache control logic interprets a memory address simply as a Tag and a Word field. The Tag field uniquely identifies a block of main memory. To determine whether a block is in the cache, the cache control logic must simultaneously examine every line’s tag for a match. \\

The downside of this is the complex circuitry needed to examine all the tags.

\subsubsection{Set Associative mapping cache}
A fully associative mapped memory can be built but is very expensive, and therefore is not done. Instead we use a \textbf{set associative} cache memory. \\
Here the cache are divided into a number of sets, where each set contains a number of slots (just like a regular cache). 


\subsubsection{Fully Associative Organization}


\subsubsection{Write Policy}
When a block in the cache is to be replaced, we have to consider two cases. If the block in the case has \textbf{not been altered}, then it can be replaced with a new block without saving the block to the main memory. Or if it \textbf{has been altered} (one block is enough), then we need to write that block back to main memory. \\

There are two cases to consider
\begin{enumerate}
\item More than one device may have access to main memory. \\
  
  For example, an I/O module may be able to read-write directly to memory. If a word has been altered only in the cache, then the corresponding memory word is invalid. Further, if the I/O device has altered main memory, then the cache word is invalid. \\
\item Multiple processors are attached to the same bus and each processor has its own local cache. Then, if a word is altered in one cache, it could conceivably invalidate a word in other caches.
\end{enumerate}
\textbf{Write through} all write operations are made to main memory as well as to the cache, ensuring that main memory is always valid. Any other processor–cache module can monitor traffic to main memory to maintain consistency within its own cache. The main disadvantage of this technique is that it generates substantial memory traffic and may create a bottleneck.\\

\textbf{Write through with buffered write} The same as write-through, but instead of slowing the processor down by writing directly to main memory, the write address and data
are stored in a high-speed write buffer; the write buffer transfers data to main memory while the processor continues its task.\\

\textbf{Write back} With write back, updates are made only in the cache. When an update occurs, a dirty bit, or use bit, associated with the line is set. Then, when a block is replaced, it is written back to main memory if and only if the dirty bit is set. \\

\subsection{Virtual memory}
Almost all nonembedded processors, and many embedded processors, support virtual memory. In essence, virtual memory is a facility that allows programs to address memory from a logical point of view, without regard to the amount of main memory physically available. When virtual memory is used, the address fields of machine instructions contain virtual addresses. For reads to and writes from main memory, a hardware memory management unit (MMU) translates each virtual address into a physical address in main memory.

\subsubsection{Paging}
Unequal fixed-size and variable-sized partitions of memory are inefficient, we need to make sure that we use the most of the memory available in the best way, we try to do this using pages.

We first divide programs into fixed sized memory blocks called pages, then we divide the main memory into equal fixed sized blocks, called page frames. \\

A process then needs a set of free page frames assigned by the OS. Then its up to the OS to make sure it keeps track of the free available frames, it also uses a page table to keep track of the mapping between pages and page frames. 

\subsubsection{Page fault}
When we try to access a piece of memory that is not currently in the main memory, but instead in the virtual memory. The OS must then load the page into the main memory firs, this is called a \textbf{Page fault}.

\subsubsection{Page replacement}
When a page fault occurs and all the page frames are occupied, one of them must be replaced. If the page that are to be replaced, has been modified, we need to write that memory back to the secondary storage. \\

We want to replace a page that we think will not be accessed for the longest amount of time. The problem is that we can't predict the future, so we have to predict the future, we can do this in some different ways, described below.

\subsubsection{Replacement algorithms}
Not needed with direct mapping, but with associative mapping we need one of these algorithms. \\

\textbf{First-in-first-out} \\
\textbf{Least recently used (LRU)}: replaces the cache that has been in the cache the longest without being referenced.\\
\textbf{Least frequently used (LFU)}: replaces the cache that has been referenced the least frequent.\\ 
\textbf{Random}\\ 
